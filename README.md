# å¤§å‹èªè¨€æ¨¡å‹æŒ‡ä»¤å¾®èª¿æµç¨‹ LLM Instruction Fine-Tuning

<!-- ![GitHub Repo stars](https://img.shields.io/github/stars/A-baoYang/LLM-FineTuning-Guide?style=social) -->
![GitHub Code License](https://img.shields.io/github/license/A-baoYang/LLM-FineTuning-Guide)
![GitHub last commit](https://img.shields.io/github/last-commit/A-baoYang/LLM-FineTuning-Guide)
![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)

æœ¬å°ˆæ¡ˆæ•´ç†äº†å¾®èª¿å¤§å‹èªè¨€æ¨¡å‹çš„é‡è¦è§€å¿µå’Œå¯¦ä½œçš„ç¨‹å¼æ¡†æ¶ï¼Œé‡å° LLMs çš„è¨“ç·´å’Œæ¨è«–æä¾›é‹è¡Œç¯„ä¾‹ã€‚

ğŸ‘‹ æ­¡è¿åŠ å…¥æˆ‘å€‘çš„ Line ç¤¾ç¾¤ Open Chatï¼š[å¤§å‹èªè¨€æ¨¡å‹å¾®èª¿åŠ OpenAI æ‡‰ç”¨è¨è«–ç¾¤](assets/line-openchat.jpg)

åˆ‡æ›èªè¨€ç‰ˆæœ¬ï¼š \[ [English](README.md) | [ç¹é«”ä¸­æ–‡](README-zhtw.md) | [ç®€ä½“ä¸­æ–‡](README-zhcn.md) \]

> å¦‚æœä½ å¸Œæœ›æ¸›å°‘è©¦éŒ¯ï¼Œæ­¡è¿å ±åæˆ‘è¦ªè‡ªéŒ„è£½çš„æ‰‹æŠŠæ‰‹æ•™å­¸èª²ç¨‹ï¼š
> - æ•™å­¸èª²ç¨‹é è³¼ï¼šhttps://dataagent.kaik.io/learning/llm-instruction-finetune

## æœ€æ–°æ¶ˆæ¯ Development Log

- [2023/04/15] æ›´æ–°æ–°è³‡æ–™é›†ï¼š

## è³‡æ–™é›† Datasets

- medical

è©³ç´°å…§å®¹è«‹æŸ¥çœ‹ [instruction-datasets/README.md](./instruction-datasets/README.md)

## æ”¯æ´çš„å¤§å‹èªè¨€æ¨¡å‹ LLMs

- LLaMA
- Bloom
- ChatGLM-6B

è©³ç´°ä»‹ç´¹è«‹æŸ¥çœ‹ [LLM ä»‹ç´¹](./docs/LLMs.md)

## é«˜æ•ˆå¾®èª¿æ–¹æ³• Efficient Parameters Fine-Tuning Methods

ç›®å‰æ”¯æ´ä»¥ä¸‹é«˜æ•ˆå¾®èª¿æ–¹å¼ï¼š

- LoRA 
- P-tuning V2

ç¡¬é«”éœ€æ±‚ï¼š

| LLM | å¾®èª¿æ–¹æ³• | é‡åŒ–æ–¹æ³• | åˆ†æ•£å¼ç­–ç•¥ | Batch Size | æ‰€éœ€ GPU è¨˜æ†¶é«”(å–®å¼µ) | é€Ÿåº¦ |
| --- | --- | --- | --- | --- | --- | --- |
| Bloom | LoRA | INT8 | None | 1 | 14GB | 86.71s/it |
| Bloom | LoRA | INT8 | Torch DDP on 2 GPUs | 1 | 13GB | 44.47s/it |
| Bloom | LoRA | INT8 | DeepSpeed ZeRO stage 3 on 2 GPUs | 1 | 13GB | 36.05s/it |
| ChatGLM-6B | P-Tuning | INT4 | DeepSpeed ZeRO stage 3 on 2 GPUs | 2 | 15GB | 14.7s/it |

---

## å¦‚ä½•é–‹å§‹ï¼Ÿ Getting Started

### è³‡æ–™é›†æº–å‚™ Data Preparation

ä½ å¯ä»¥é¸æ“‡ä½¿ç”¨é–‹æºæˆ–å­¸è¡“è³‡æ–™é›†é€²è¡Œå¾®èª¿ï¼›ä½†å¦‚æœé–‹æºè³‡æ–™é›†ä¸ç¬¦åˆæ‚¨çš„æ‡‰ç”¨æƒ…å¢ƒï¼Œæ‚¨å°±æœƒéœ€è¦ä½¿ç”¨è‡ªå®šç¾©è³‡æ–™é›†ä¾†é€²è¡Œã€‚

åœ¨æœ¬å°ˆæ¡ˆè³‡æ–™é›†æ‰€ä½¿ç”¨æ ¼å¼æ˜¯ `.json` ï¼Œä½ æœƒéœ€è¦å°‡è³‡æ–™é›† train, dev, test åˆ†éš”å¾Œçš„æª”æ¡ˆæ”¾åˆ° `instruction-datasets/` ä¸‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥å¦å¤–å‰µæ–°è³‡æ–™å¤¾æ”¾ç½®ï¼Œåªæ˜¯è·¯å¾‘æŒ‡å®šçš„å·®ç•°ï¼Œéƒ½å¯ä»¥åœ¨ commands åšä¿®æ”¹ã€‚

### ç’°å¢ƒæº–å‚™ Requirements

é‡å°ä¸åŒçš„å¾®èª¿æ–¹æ³•æœ‰è¨­å®šå¥½æ‰€éœ€çš„å¥—ä»¶ï¼Œåªè¦é€²åˆ°æœ‰ `requirements.txt` çš„è³‡æ–™å¤¾ä¸‹é‹è¡Œ

```bash
git clone https://github.com/A-baoYang/LLM-FineTuning-Guide.git
conda create -n llm_ift python=3.8
conda activate llm_ift
cd LLM-Finetune-Guide/efficient-finetune/ptuning/v2
pip install -r requirements.txt
```

## å¾®èª¿ Fine-Tuning

è³‡æ–™æº–å‚™å¥½ä¹‹å¾Œå°±å¯ä»¥å•Ÿå‹•å¾®èª¿ï¼Œé€™è£¡å·²ç¶“å°‡ç¨‹å¼å¯«å¥½ï¼Œç•¶ä¸­çš„è³‡æ–™/æ¨¡å‹è·¯å¾‘ã€åƒæ•¸ç½®æ›éƒ½å¯ä»¥é€éæŒ‡ä»¤ä¾†æŒ‡å®šã€‚

### å–®å¼µ GPU å¾®èª¿ Fine-Tuning with single GPU

```bash
CUDA_VISIBLE_DEVICES=0 python finetune.py \
    --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR
```

å®Œæ•´çš„åƒæ•¸å’ŒæŒ‡ä»¤è¨­å®šè«‹è¦‹ï¼š [finetune.sh](./efficient-finetune/ptuning/v2/finetune.sh)

### å¤šå¼µ GPU å¾®èª¿ Fine-Tuning with multiple GPUs

- ä½¿ç”¨ torchrun å•Ÿå‹•

```bash
torchrun --standalone --nnodes=1  --nproc_per_node=2 finetune.py --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

å®Œæ•´çš„åƒæ•¸å’ŒæŒ‡ä»¤è¨­å®šè«‹è¦‹ï¼š [finetune-ddp.sh](./efficient-finetune/ptuning/v2/finetune-ddp.sh)

- ä½¿ç”¨ accelerate å•Ÿå‹•

```bash
accelerate launch finetune.py --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

### ä½¿ç”¨ DeepSpeed ZeRO ç­–ç•¥é€²è¡Œåˆ†æ•£å¼è¨“ç·´

- ä½¿ç”¨ accelerate å¸¶ä¸Š config_file å•Ÿå‹•

```bash
accelerate launch --config_file ../../config/use_deepspeed.yaml finetune.py --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

- ä½¿ç”¨ deepspeed å•Ÿå‹•

```bash
deepspeed --num_nodes 1 --num_gpus 2 finetune.py \
    --deepspeed ../../config/zero_stage3_offload_config.json \
    --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

- æ›´å¤šå¾®èª¿æ¡ˆä¾‹è«‹çœ‹ï¼š[efficient-finetune/README.md](./efficient-finetune/README.md)

## æ¨¡å‹è©•ä¼°èˆ‡é æ¸¬ Evaluation & Prediction

```bash
CUDA_VISIBLE_DEVICES=0 python finetune.py \
    --do_predict \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --test_file ../../../instruction-datasets/$DATATAG/test.json \
    --overwrite_cache \
    --prompt_column input \
    --response_column output \
    --model_name_or_path $MODEL_PATH \
    --ptuning_checkpoint finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR/checkpoint-$STEP \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR
```

## æ¨¡å‹æ¨è«– Run Inference

- çµ‚ç«¯æ©Ÿ

```bash
cd LLM-Finetune-Guide/efficient-finetune/ptuning/v2/serve/
CUDA_VISIBLE_DEVICES=0 python cli_demo.py \
    --pretrained_model_path THUDM/chatglm-6b \
    --ptuning_checkpoint ../finetuned/chatglm-6b-pt-512-2e-2/checkpoint-3000 \
    --is_cuda True
```

- ç¶²é å±•ç¤º
```bash
cd LLM-Finetune-Guide/efficient-finetune/lora/serve/
python ui.py
```

- Model API

```bash
cd LLM-Finetune-Guide/efficient-finetune/lora/serve/
python api.py
```

## åœ¨ CPU ç’°å¢ƒä¸‹æé€Ÿé‹è¡Œ

æœ€å¾Œç‰¹åˆ¥æŠŠ CPU æå‡ºä¾†è¬›ï¼Œå› ç‚ºå¦‚æœå¯ä»¥åšåˆ°åœ¨ CPU ç’°å¢ƒä¸‹é‹è¡Œ finetune éçš„å¤§èªè¨€æ¨¡å‹ï¼Œ
æœƒæœ€å¤§æ¯”ä¾‹çš„ç¯€çœé‹ç®—æˆæœ¬ã€‚é€™ä¸€å¡Šç›®å‰æ–¹æ³•æœ‰ï¼š

- å°‡æ¨¡å‹æª”è½‰æ›ç‚º cpp æ ¼å¼
    - LLaMA + LoRA -> llama.cpp
    - Bloom + LoRA -> bloom.cpp

- ä½¿ç”¨ INT4 æ–¼ CPU ç’°å¢ƒé‹è¡Œï¼Œé€Ÿåº¦å¯æ¥å—

```bash
cd LLM-Finetune-Guide/efficient-finetune/ptuning/v2/serve/
CUDA_VISIBLE_DEVICES=0 python cli_demo.py \
    --pretrained_model_path THUDM/chatglm-6b \
    --ptuning_checkpoint ../finetuned/chatglm-6b-pt-512-2e-2/checkpoint-3000 \
    --quantization_bit 4 \
    --is_cuda True
```

## TODO

- [ ] ç¨‹å¼ç¢¼é‡æ§‹

---

## License

- å°ˆæ¡ˆ Licenseï¼š[Apache-2.0 License](./LICENSE)
- æ¨¡å‹ Licenseï¼šè«‹åƒç…§å„å¤§èªè¨€æ¨¡å‹æ‰€æä¾›ä¹‹ Licenseï¼Œè©³ç´°è«‹è¦‹ [LLM ä»‹ç´¹](./docs/LLMs.md)

## Citation

å¦‚æœé€™é …å°ˆæ¡ˆå°ä½ çš„å·¥ä½œæˆ–ç ”ç©¶æœ‰å¹«åŠ©ï¼Œè«‹å¼•ç”¨ï¼š

```
@Misc{LLM-FineTuning-Guide,
  title = {LLM FineTuning Guide},
  author = {A-baoYang},
  howpublished = {\url{https://github.com/A-baoYang/LLM-Finetuning-Guide}},
  year = {2023}
}
```

## Acknowledgement

æ­¤å°ˆæ¡ˆå¾ä»¥ä¸‹åœ°æ–¹ç²å–éˆæ„Ÿï¼Œæ„Ÿè¬é€™äº›å¾ˆè®šçš„å°ˆæ¡ˆï¼š

- [THUDM/ChatGLM-6B]
- [ymcui/Chinese-LLaMA-Alpaca]
- [tloen/alpaca-lora]
