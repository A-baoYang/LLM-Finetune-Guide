# å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒæµç¨‹ LLM Instruction Fine-Tuning

![GitHub Repo stars](https://img.shields.io/github/stars/A-baoYang/LLM-FineTuning-Guide?style=social)
![GitHub Code License](https://img.shields.io/github/license/A-baoYang/LLM-FineTuning-Guide)
![GitHub last commit](https://img.shields.io/github/last-commit/A-baoYang/LLM-FineTuning-Guide)
![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)

æœ¬ä¸“æ¡ˆæ•´ç†äº†å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦è§‚å¿µå’Œå®ä½œçš„ç¨‹å¼æ¡†æ¶ï¼Œé’ˆå¯¹ LLMs çš„è®­ç»ƒå’Œæ¨è®ºæä¾›è¿è¡ŒèŒƒä¾‹ã€‚

ğŸ‘‹ æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ Line ç¤¾ç¾¤ Open Chatï¼š[å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒåŠ OpenAI åº”ç”¨è®¨è®ºç¾¤](assets/line-openchat.jpg)

åˆ‡æ¢è¯­è¨€ç‰ˆæœ¬ï¼š \[ [English](README.md) | [ç¹ä½“ä¸­æ–‡](README-zhtw.md) | [ç®€ä½“ä¸­æ–‡](README-zhcn.md) \]

> å¦‚æœä½ å¸Œæœ›å‡å°‘è¯•é”™ï¼Œæ¬¢è¿æŠ¥åæˆ‘äº²è‡ªå½•è£½çš„æ‰‹æŠŠæ‰‹æ•™å­¦è¯¾ç¨‹ï¼š
> - å¡«å†™é—®å·é¢†å–ä¼˜æƒ æŠ˜æ‰£ï¼š [https://www.surveycake.com/s/kn0bL](https://www.surveycake.com/s/kn0bL)

<!-- ## æœ€æ–°æ¶ˆæ¯ Development Log

- [2023/04/15] æ›´æ–°æ–°èµ„æ–™é›†ï¼š -->

<!-- ## èµ„æ–™é›† Datasets

- medical

è¯¦ç»†å†…å®¹è¯·æŸ¥çœ‹ [instruction-datasets/README.md](./instruction-datasets/README.md)

## æ”¯æ´çš„å¤§å‹è¯­è¨€æ¨¡å‹ LLMs

- LLaMA
- Bloom
- ChatGLM-6B

è¯¦ç»†ä»‹ç»è¯·æŸ¥çœ‹ [LLM ä»‹ç»](./docs/LLMs.md) -->

## é«˜æ•ˆå¾®è°ƒæ–¹æ³• Efficient Parameters Fine-Tuning Methods

ç›®å‰æ”¯æ´ä»¥ä¸‹é«˜æ•ˆå¾®è°ƒæ–¹å¼ï¼š

- LoRA 
- P-tuning V2

ç¡¬ä½“éœ€æ±‚ï¼š

| LLM | å¾®è°ƒæ–¹æ³• | é‡åŒ–æ–¹æ³• | åˆ†æ•£å¼ç­–ç•¥ | Batch Size | æ‰€éœ€ GPU è®°å¿†ä½“(å•å¼ ) | é€Ÿåº¦ |
| --- | --- | --- | --- | --- | --- | --- |
| Bloom | LoRA | INT8 | None | 1 | 14GB | 86.71s/it |
| Bloom | LoRA | INT8 | Torch DDP on 2 GPUs | 1 | 13GB | 44.47s/it |
| Bloom | LoRA | INT8 | DeepSpeed ZeRO stage 3 on 2 GPUs | 1 | 13GB | 36.05s/it |
| ChatGLM-6B | P-Tuning | INT4 | DeepSpeed ZeRO stage 3 on 2 GPUs | 2 | 15GB | 14.7s/it |

---

## å¦‚ä½•å¼€å§‹ï¼Ÿ Getting Started

### èµ„æ–™é›†å‡†å¤‡ Data Preparation

ä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨å¼€æºæˆ–å­¦æœ¯èµ„æ–™é›†è¿›è¡Œå¾®è°ƒï¼›ä½†å¦‚æœå¼€æºèµ„æ–™é›†ä¸ç¬¦åˆæ‚¨çš„åº”ç”¨æƒ…å¢ƒï¼Œæ‚¨å°±ä¼šéœ€è¦ä½¿ç”¨è‡ªå®šä¹‰èµ„æ–™é›†æ¥è¿›è¡Œã€‚

åœ¨æœ¬ä¸“æ¡ˆèµ„æ–™é›†æ‰€ä½¿ç”¨æ ¼å¼æ˜¯ `.json` ï¼Œä½ ä¼šéœ€è¦å°†èµ„æ–™é›† train, dev, test åˆ†éš”åçš„æ¡£æ¡ˆæ”¾åˆ° `instruction-datasets/` ä¸‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥å¦å¤–åˆ›æ–°èµ„æ–™å¤¹æ”¾ç½®ï¼Œåªæ˜¯è·¯å¾„æŒ‡å®šçš„å·®å¼‚ï¼Œéƒ½å¯ä»¥åœ¨ commands åšä¿®æ”¹ã€‚

### ç¯å¢ƒå‡†å¤‡ Requirements

é’ˆå¯¹ä¸åŒçš„å¾®è°ƒæ–¹æ³•æœ‰è®¾å®šå¥½æ‰€éœ€çš„å¥—ä»¶ï¼Œåªè¦è¿›åˆ°æœ‰ `requirements.txt` çš„èµ„æ–™å¤¹ä¸‹è¿è¡Œ

```bash
git clone https://github.com/A-baoYang/LLM-FineTuning-Guide.git
conda create -n llm_ift python=3.8
conda activate llm_ift
cd LLM-Finetune-Guide/efficient-finetune/ptuning/v2
pip install -r requirements.txt
```

## å¾®è°ƒ Fine-Tuning

èµ„æ–™å‡†å¤‡å¥½ä¹‹åå°±å¯ä»¥å¯åŠ¨å¾®è°ƒï¼Œè¿™è£¡å·²ç»å°†ç¨‹å¼å†™å¥½ï¼Œå½“ä¸­çš„èµ„æ–™/æ¨¡å‹è·¯å¾„ã€å‚æ•°ç½®æ¢éƒ½å¯ä»¥é€è¿‡æŒ‡ä»¤æ¥æŒ‡å®šã€‚

### å•å¼  GPU å¾®è°ƒ Fine-Tuning with single GPU

```bash
CUDA_VISIBLE_DEVICES=0 python finetune.py \
    --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR
```

å®Œæ•´çš„å‚æ•°å’ŒæŒ‡ä»¤è®¾å®šè¯·è§ï¼š [finetune.sh](./efficient-finetune/ptuning/v2/finetune.sh)

### å¤šå¼  GPU å¾®è°ƒ Fine-Tuning with multiple GPUs

- ä½¿ç”¨ torchrun å¯åŠ¨

```bash
torchrun --standalone --nnodes=1  --nproc_per_node=2 finetune.py --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

å®Œæ•´çš„å‚æ•°å’ŒæŒ‡ä»¤è®¾å®šè¯·è§ï¼š [finetune-ddp.sh](./efficient-finetune/ptuning/v2/finetune-ddp.sh)

- ä½¿ç”¨ accelerate å¯åŠ¨

```bash
accelerate launch finetune.py --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

### ä½¿ç”¨ DeepSpeed ZeRO ç­–ç•¥è¿›è¡Œåˆ†æ•£å¼è®­ç»ƒ

- ä½¿ç”¨ accelerate å¸¦ä¸Š config_file å¯åŠ¨

```bash
accelerate launch --config_file ../../config/use_deepspeed.yaml finetune.py --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

- ä½¿ç”¨ deepspeed å¯åŠ¨

```bash
deepspeed --num_nodes 1 --num_gpus 2 finetune.py \
    --deepspeed ../../config/zero_stage3_offload_config.json \
    --do_train \
    --train_file ../../../instruction-datasets/$DATATAG/train.json \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --prompt_column input \
    --response_column output \
    --overwrite_cache \
    --model_name_or_path $MODEL_PATH \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR \
```

- æ›´å¤šå¾®è°ƒæ¡ˆä¾‹è¯·çœ‹ï¼š[efficient-finetune/README.md](./efficient-finetune/README.md)

## æ¨¡å‹è¯„ä¼°ä¸é¢„æµ‹ Evaluation & Prediction

```bash
CUDA_VISIBLE_DEVICES=0 python finetune.py \
    --do_predict \
    --validation_file ../../../instruction-datasets/$DATATAG/dev.json \
    --test_file ../../../instruction-datasets/$DATATAG/test.json \
    --overwrite_cache \
    --prompt_column input \
    --response_column output \
    --model_name_or_path $MODEL_PATH \
    --ptuning_checkpoint finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR/checkpoint-$STEP \
    --output_dir finetuned/$DATATAG-$MODEL_TYPE-pt-$PRE_SEQ_LEN-$LR
```

## æ¨¡å‹æ¨è®º Run Inference

- ç»ˆç«¯æœº

```bash
cd LLM-Finetune-Guide/efficient-finetune/ptuning/v2/serve/
CUDA_VISIBLE_DEVICES=0 python cli_demo.py \
    --pretrained_model_path THUDM/chatglm-6b \
    --ptuning_checkpoint ../finetuned/chatglm-6b-pt-512-2e-2/checkpoint-3000 \
    --is_cuda True
```

- ç½‘é¡µå±•ç¤º
```bash
cd LLM-Finetune-Guide/efficient-finetune/lora/serve/
python ui.py
```

- Model API

```bash
cd LLM-Finetune-Guide/efficient-finetune/lora/serve/
python api.py
```

## åœ¨ CPU ç¯å¢ƒä¸‹æé€Ÿè¿è¡Œ

å¦‚æœå¯ä»¥åšåˆ°åœ¨ CPU ç¯å¢ƒä¸‹è¿è¡Œ finetune è¿‡çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä¼šæœ€å¤§æ¯”ä¾‹çš„é™ä½ LLM çš„åº”ç”¨é—¨æ§›ã€‚

- ä½¿ç”¨ INT4 äº CPU ç¯å¢ƒè¿è¡Œï¼Œé€Ÿåº¦å¯æ¥å—

```bash
cd LLM-Finetune-Guide/efficient-finetune/ptuning/v2/serve/
CUDA_VISIBLE_DEVICES=0 python cli_demo.py \
    --pretrained_model_path THUDM/chatglm-6b \
    --ptuning_checkpoint ../finetuned/chatglm-6b-pt-512-2e-2/checkpoint-3000 \
    --quantization_bit 4 \
    --is_cuda True
```

---

## License

- ä¸“æ¡ˆ Licenseï¼š[Apache-2.0 License](./LICENSE)
- æ¨¡å‹ Licenseï¼šè¯·å‚ç…§å„å¤§è¯­è¨€æ¨¡å‹æ‰€æä¾›ä¹‹ License
<!-- è¯¦ç»†è¯·è§ [LLM ä»‹ç»](./docs/LLMs.md) -->

## Citation

å¦‚æœè¿™é¡¹ä¸“æ¡ˆå¯¹ä½ çš„å·¥ä½œæˆ–ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```
@Misc{LLM-Finetune-Guide,
  title = {LLM Finetune Guide},
  author = {A-baoYang},
  howpublished = {\url{https://github.com/A-baoYang/LLM-Finetune-Guide}},
  year = {2023}
}
```

## Acknowledgement

æ­¤ä¸“æ¡ˆä»ä»¥ä¸‹åœ°æ–¹è·å–çµæ„Ÿï¼Œæ„Ÿè°¢è¿™äº›å¾ˆè®šçš„ä¸“æ¡ˆï¼š

- [THUDM/ChatGLM-6B]
- [ymcui/Chinese-LLaMA-Alpaca]
- [tloen/alpaca-lora]

## Contact

æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æ¥ä¿¡è¯¢é—®ï¼š [jiunyi.yang.abao@gmail.com](mailto:jiunyi.yang.abao@gmail.com)